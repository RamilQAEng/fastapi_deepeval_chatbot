# Обзор Проекта (Project Overview)

Этот документ описывает архитектуру, ключевые компоненты и процессы разработки проекта. Он служит единой точкой входа для понимания устройства системы.

---

## 1. Архитектура и FastAPI

Наш бэкенд построен на **FastAPI**. Это современный, высокопроизводительный асинхронный фреймворк.

### Структура `src/`
*   `api/` — Внешний интерфейс (API Layer).
    *   `main.py`: Точка входа приложения.
    *   `endpoints/rag.py`: Маршруты API (endpoints) для RAG-функционала (`/generate`, `/upload`, `/evaluations`).
*   `services/` — Бизнес-логика (Service Layer).
    *   `dataset_service.py`: Логика генерации и управления датасетами.
    *   `evaluation_service.py`: Логика запуска оценок (DeepEval) и расчета метрик.
*   `models/` — Описание таблиц базы данных (SQLAlchemy Models).
*   `schemas/` — Pydantic модели для валидации данных (DTOs).

### Процесс добавления нового эндпоинта
1.  Опишите Pydantic-схему в `src/schemas/`.
2.  Реализуйте бизнес-логику в соответствующем сервисе (`src/services/`).
3.  Зарегистрируйте маршрут в `src/api/endpoints/rag.py`.

---

## 2. База Данных (PostgreSQL + Alembic)

Проект использует **PostgreSQL** в качестве основного хранилища данных.
Взаимодействие с БД осуществляется через **SQLAlchemy** (ORM), а управление схемой — через **Alembic**.

*   **Модели**: Расположены в `src/models/evaluation.py`. Любые изменения структуры данных должны начинаться здесь.
*   **Миграции**: Для применения изменений в БД необходимо создавать миграции (`alembic revision --autogenerate`).
    *   *Подробнее см. в [ONBOARDING_DB.md](./ONBOARDING_DB.md)*.

---

## 3. Оценка качества (DeepEval)

Для автоматизированного тестирования качества RAG-системы используется фреймворк **DeepEval**.

### Основные метрики

1.  **Faithfulness (Достоверность)**:
    *   Проверяет, не противоречит ли ответ модели найденному контексту.
    *   Целевое значение: 1.0 (Ответ полностью основан на контексте).

2.  **Answer Relevancy (Релевантность ответа)**:
    *   Оценивает, насколько ответ соответствует заданному вопросу, игнорируя контекст.
    *   Целевое значение: 1.0 (Ответ четкий и по существу).

### Локализация (Русский язык)
В файле `src/metrics/russian.py` реализована адаптация промптов DeepEval для работы с русским языком. Это обеспечивает корректную оценку ответов на кириллице.

---

## 4. Генерация Датасетов

Система поддерживает автоматическую генерацию синтетических тестовых данных.

*   **Реализация**: `src/services/dataset_service.py` -> `generate_synthetic_from_text`.
*   **Принцип работы**:
    1.  Текст источника отправляется в LLM (GPT-4).
    2.  Модель генерирует сложные вопросы и эталонные ответы.
    3.  Результат сохраняется как новый `Dataset`.

---

## 5. Формат Данных (JSON)

Для импорта внешних датасетов необходимо соблюдать строгую структуру JSON.

### Структура `DatasetConfig`

```json
{
  "test_cases": [
    {
      "input": "Вопрос пользователя",
      "actual_output": "Ответ системы (для оценки)",
      "expected_output": "Эталонный ответ (Golden Answer)",
      "context": ["Исходный текст для генерации ответа"],
      "retrieval_context": ["Фрагменты, найденные поиском RAG"]
    }
  ]
}
```

### Описание полей
*   `input`: **Обязательно**. Текст вопроса.
*   `actual_output`: **Обязательно для оценки**. Ответ, который нужно оценить.
*   `retrieval_context`: **Критично для Faithfulness**. Список фрагментов текста, использованных для ответа. Если поле пустое, оценка достоверности невозможна.
*   `expected_output`: Необязательно. Используется для метрик Correctness.

### Получение шаблона
Запрос `GET /api/v1/template` возвращает пустой JSON-шаблон для заполнения.

---

## 6. Кастомные Метрики (G-Eval)

Для оценки субъективных качеств (например, "Вежливость", "Безопасность") используется подход G-Eval. Метрика определяется через описание критериев на естественном языке.

*   Подробнее см. в [DEVELOPMENT_GUIDE.md](./DEVELOPMENT_GUIDE.md).

---

## 7. Тестирование и Верификация

Каталог `tests/` содержит скрипты для проверки работоспособности системы без использования UI.

### Ключевые скрипты:

1.  **`tests/verify_pipeline.py`**
    *   **Назначение**: End-to-End тестирование.
    *   **Процесс**: Генерация датасета -> Оценка -> Проверка статуса.
    *   **Цель**: Убедиться в работоспособности всей цепочки компонентов.

2.  **`tests/verify_mixed_quality.py`**
    *   **Назначение**: Тестирование метрик качества.
    *   **Процесс**: Создание датасета с заведомо "плохими" и "хорошими" ответами.
    *   **Цель**: Проверить калибровку метрик Faithfulness и Answer Relevancy.

3.  **`tests/verify_analytics.py`**
    *   **Назначение**: Верификация аналитических данных.
    *   **Процесс**: Проверка корректности сохранения метрик, времени выполнения и статистики в БД.
    *   **Цель**: Гарантировать точность данных для аналитических отчетов.

### Экспорт Отчетов (XLSX / CSV)

Система поддерживает выгрузку результатов оценки в форматах Excel и CSV.

*   **Endpoint**: `GET /api/v1/evaluations/{run_id}/download?format=xlsx`
*   **Проверка**:
    1.  Запустите сервер локально.
    2.  Выполните прогон оценки (через тесты или API), получив `run_id`.
    3.  Выполните GET-запрос к эндпоинту скачивания.
    4.  Результат: Excel-файл со сводной статистикой (Summary) и детальными данными (Details).
